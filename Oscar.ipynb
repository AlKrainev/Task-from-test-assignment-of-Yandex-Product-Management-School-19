{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading file\n",
    "oscar_df = pd.read_csv('oscar.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normal_query</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neityfz говядина как консервы</td>\n",
       "      <td>2019-01-23 09:39:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>водокаал алексин</td>\n",
       "      <td>2019-01-23 09:39:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>настя шевцова и гуф</td>\n",
       "      <td>2019-01-23 09:39:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ул барсова 7 сочи</td>\n",
       "      <td>2019-01-23 09:39:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>доп соглашение к срочному трудовому договору о...</td>\n",
       "      <td>2019-01-23 09:39:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        normal_query             datetime\n",
       "0                      neityfz говядина как консервы  2019-01-23 09:39:25\n",
       "1                                   водокаал алексин  2019-01-23 09:39:25\n",
       "2                                настя шевцова и гуф  2019-01-23 09:39:25\n",
       "3                                  ул барсова 7 сочи  2019-01-23 09:39:25\n",
       "4  доп соглашение к срочному трудовому договору о...  2019-01-23 09:39:25"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see whats the data we're working with\n",
    "oscar_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At 1st, let's transform the 'datetime' column into datetime format\n",
    "oscar_df['datetime'] = pd.to_datetime(oscar_df['datetime'])\n",
    "\n",
    "# Then let's divide whole DF into 3 parts, according to Oscar ceremony date (2019-02-25): before/at/after parts; \n",
    "# and cleaning the data from missing walues at the same time \n",
    "oscar_df_before = oscar_df[oscar_df['datetime'] < pd.to_datetime('2019-02-25')].dropna(axis = 0)\n",
    "oscar_df_at = oscar_df[oscar_df['datetime'] == pd.to_datetime('2019-02-25')].dropna(axis = 0)\n",
    "oscar_df_after = oscar_df[oscar_df['datetime'] > pd.to_datetime('2019-02-26')].dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 'Before Oscar' 34616468 rows\n",
      "Part 'At Oscar' 6 rows\n",
      "Part 'After Oscar' 3193689 rows\n"
     ]
    }
   ],
   "source": [
    "# Lets see the size of each part\n",
    "print(\"Part 'Before Oscar'\", oscar_df_before.shape[0], \"rows\"), \n",
    "print(\"Part 'At Oscar'\", oscar_df_at.shape[0], \"rows\"),\n",
    "print(\"Part 'After Oscar'\", oscar_df_after.shape[0], \"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to feed the CountVectorizer with data correctly, lets take only the 'normal query' columns \n",
    "# and transform it into lists of queries   \n",
    "oscar_lst_b = oscar_df_before['normal_query'].tolist()\n",
    "oscar_lst_at = oscar_df_at['normal_query'].tolist()\n",
    "oscar_lst_af = oscar_df_after['normal_query'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets prepare data for LDA. Lets make sparse matrices from each part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features = n_features)\n",
    "\n",
    "tf_b = tf_vectorizer.fit_transform(oscar_lst_b)\n",
    "tf_feature_names_b = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=1, max_features = n_features)\n",
    "\n",
    "tf_at = tf_vectorizer.fit_transform(oscar_lst_at)\n",
    "tf_feature_names_at = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features = n_features)\n",
    "\n",
    "tf_af = tf_vectorizer.fit_transform(oscar_lst_af)\n",
    "tf_feature_names_af = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets fit the LDA to each part separatly and see what time it will take**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 15min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_topics = 5\n",
    "lda_b = LatentDirichletAllocation(n_components=n_topics,\\\n",
    "                                  max_iter=5, learning_method='online',\\\n",
    "                                  learning_offset=50.0,random_state=84).fit(tf_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_topics = 5\n",
    "lda_at = LatentDirichletAllocation(n_components=n_topics,\\\n",
    "                                  max_iter=5, learning_method='online',\\\n",
    "                                  learning_offset=50.0,random_state=84).fit(tf_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_topics = 5\n",
    "lda_af = LatentDirichletAllocation(n_components=n_topics,\\\n",
    "                                  max_iter=5, learning_method='online',\\\n",
    "                                  learning_offset=50.0,random_state=84).fit(tf_af)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making function for the output\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets group all in 5 topics for each of 3 parts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics of the queries was made <font color = 'green'> **BEFORE** </font> the day of Oscar ceremony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: не видео вк одноклассники страница про моя где москва дом\n",
      "Topic #1: порно это сезон 10 секс магазин серия февраля интернет песня\n",
      "Topic #2: на для онлайн купить 2019 2018 отзывы по за цена\n",
      "Topic #3: как на сайт официальный что из от ру ru сколько\n",
      "Topic #4: скачать по смотреть фильм на бесплатно класс фото погода сериал\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda_b, tf_feature_names_b, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics of the queries was made <font color = 'green'> **AT** </font> the day of Oscar ceremony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: самара на погода катя преследует индивидуум gta уилл womeximizer адушкина\n",
      "Topic #1: катя адушкина womeximizer gta феррелл уилл самара индивидуум каждый английском\n",
      "Topic #2: уилл womeximizer феррелл gta самара на английском индивидуум преследует погода\n",
      "Topic #3: gta womeximizer индивидуум преследует каждый катя феррелл погода английском адушкина\n",
      "Topic #4: английском преследует каждый индивидуум погода на катя феррелл самара уилл\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda_at, tf_feature_names_at, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics of the queries was made <font color = 'green'> **AFTER** </font> the day of Oscar ceremony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: порно сайт официальный отзывы ру видео вк авито секс страница\n",
      "Topic #1: для скачать онлайн смотреть бесплатно все без области спб качестве\n",
      "Topic #2: по как класс купить что от это гдз россии про\n",
      "Topic #3: на из за при цена погода ли можно работа сегодня\n",
      "Topic #4: 2019 фильм 2018 фото не ru сезон сколько сериал 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda_af, tf_feature_names_af, n_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
